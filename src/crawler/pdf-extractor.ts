/**
 * PDF æ–‡æ¡£æå–å™¨
 */

import fs from 'fs-extra';
import pdfParse from 'pdf-parse';
import path from 'path';
import type { PDFExtractionOptions, PDFExtractionResult, CrawledPage } from './types.js';

export class PDFExtractor {
  /**
   * ä» PDF æ–‡ä»¶æå–å†…å®¹
   */
  async extractFromPDF(
    pdfPath: string,
    options: PDFExtractionOptions = {}
  ): Promise<PDFExtractionResult> {
    console.log(`ğŸ“„ å¼€å§‹æå– PDF: ${pdfPath}`);

    // è¯»å– PDF æ–‡ä»¶
    const dataBuffer = await fs.readFile(pdfPath);

    // åŸºç¡€æå–
    const data = await pdfParse(dataBuffer);

    const result: PDFExtractionResult = {
      text: data.text,
      pages: data.numpages,
      metadata: data.metadata || {}
    };

    console.log(`âœ… PDF æå–å®Œæˆ: ${result.pages} é¡µ`);

    return result;
  }

  /**
   * å°† PDF æå–ç»“æœè½¬æ¢ä¸º CrawledPage æ ¼å¼
   */
  convertToCrawledPage(
    pdfPath: string,
    extractionResult: PDFExtractionResult,
    name: string
  ): CrawledPage {
    const title = extractionResult.metadata?.Title || 
                  path.basename(pdfPath, '.pdf');

    // åˆ†å‰²æ–‡æœ¬ä¸ºæ®µè½
    const paragraphs = extractionResult.text
      .split('\n\n')
      .filter(p => p.trim().length > 0);

    // è½¬æ¢ä¸º Markdown
    const content = paragraphs
      .map(p => p.trim().replace(/\n/g, ' '))
      .join('\n\n');

    // æå–ä»£ç å—ï¼ˆç®€å•å¯å‘å¼ï¼‰
    const codeExamples = this.extractCodeFromText(extractionResult.text);

    return {
      url: `file://${pdfPath}`,
      title,
      content,
      htmlContent: `<pre>${extractionResult.text}</pre>`,
      codeExamples,
      category: 'pdf-document',
      scrapedAt: new Date().toISOString(),
      links: []
    };
  }

  /**
   * ä»æ–‡æœ¬ä¸­æå–ä»£ç å—
   */
  private extractCodeFromText(text: string): Array<{
    language: string;
    content: string;
    lineCount: number;
  }> {
    const codeExamples: Array<{
      language: string;
      content: string;
      lineCount: number;
    }> = [];

    // æ£€æµ‹ä»£ç å—æ¨¡å¼
    const codePatterns = [
      // å¸¸è§çš„ä»£ç æ ‡è®°
      /```(\w+)?\n([\s\S]+?)```/g,
      // ç¼©è¿›çš„ä»£ç å—
      /((?:^    .+$\n?)+)/gm
    ];

    for (const pattern of codePatterns) {
      let match;
      while ((match = pattern.exec(text)) !== null) {
        const language = match[1] || 'text';
        const content = (match[2] || match[1] || '').trim();
        
        if (content.length > 10 && content.length < 5000) {
          codeExamples.push({
            language,
            content,
            lineCount: content.split('\n').length
          });
        }
      }
    }

    return codeExamples.slice(0, 20);
  }

  /**
   * ä¿å­˜æå–ç»“æœ
   */
  async saveExtraction(
    outputDir: string,
    name: string,
    page: CrawledPage
  ): Promise<void> {
    await fs.ensureDir(outputDir);

    // ä¿å­˜ä¸º JSON
    const pagesDir = path.join(outputDir, 'pages');
    await fs.ensureDir(pagesDir);
    await fs.writeJSON(
      path.join(pagesDir, '001-pdf-content.json'),
      page,
      { spaces: 2 }
    );

    // ä¿å­˜æ‘˜è¦
    const summary = {
      name,
      baseUrl: page.url,
      totalPages: 1,
      categories: { 'pdf-document': 1 },
      crawledAt: page.scrapedAt
    };

    await fs.writeJSON(
      path.join(outputDir, 'summary.json'),
      summary,
      { spaces: 2 }
    );

    // ä¿å­˜ä¸º Markdown
    const mdPath = path.join(outputDir, `${name}.md`);
    await fs.writeFile(mdPath, `# ${page.title}\n\n${page.content}`);

    console.log(`ğŸ’¾ PDF æå–ç»“æœå·²ä¿å­˜: ${outputDir}`);
  }
}

